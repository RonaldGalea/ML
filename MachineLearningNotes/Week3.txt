Logistic regression - the algorithm for classification problems

The classification problem involves setting a descision boundary. In the linear case, an example would be points that are on a certain side of a line (theta'*x>=0). Note that the sigmoid function gives >= 1/2 for positive arguments, hence points on the positive side of the line would be regarded as 1 (cuz of >=50% probability). A more general example would be the inside and outside region of a circle.

Theta'*x represents our boundary shape equation, depending on where the point lies (t'*x > 0 or t'*x <0)
we assign it a 0 or 1 

!Important note on the cost function: Due to the nonlinearity of the sigmoid function, if we considered the cost function to be the same as the one from linear regression, it would turn out to not be convex

So to make it convex, we use the following trick:
!Cost(h(x),y) = -log(h(x)) if y = 1, -log(1-h(x)) if y = 0; instead of simply Cost(h(x),y) = (h(x)-y)^2; 

So for logistic regression: Cost(h(x),y) = -y*log(h(x)) -(1-y)log(1-h(x)), where h(x) = 1/(1-e^theta'*x)
and the cost function is the sum over all m training examples of this cost: Sum(Cost(h(xi),yi))

From here on the problem is identical to the linear regression problem, simply minimize this cost function with gradient descent



Overfitting/High variance
- the hypothesis fits the training set very well (too well), but fails to generalize to new examples

Solution => Regulatization - making sure the parameters theta are small valued => this for some reason gives smoother hypothesis functions, less prone to overfitting
The key to do this is modifying the cost function by adding a regularization term: Simply add lambda*sum(theta.^2), this will penalize high
values of theta params